{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"d:\\python\\lib\\site-packages\")\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "from config import *\n",
    "from utils.graph_utils import *\n",
    "from utils.google_tsp_reader import GoogleTSPReader\n",
    "from utils.plot_utils import *\n",
    "from models.gcn_model import ResidualGatedGCNModel\n",
    "from utils.model_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "config_path = \"configs/tsp100.json\"\n",
    "config = get_config(config_path)\n",
    "#print(\"Loaded {}:\\n{}\".format(config_path, config))\n",
    "dtypeFloat = torch.FloatTensor\n",
    "dtypeLong = torch.LongTensor\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the network\n",
    "net = nn.DataParallel(ResidualGatedGCNModel(config, dtypeFloat, dtypeLong))\n",
    "learning_rate = config.learning_rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "epoch_bar = master_bar(range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_(net, optimizer, config, master_bar):#train 1 epoch\n",
    "    # Set training mode\n",
    "    net.train()\n",
    "\n",
    "    # Assign parameters\n",
    "    num_nodes = config.num_nodes\n",
    "    num_neighbors = config.num_neighbors\n",
    "    batch_size = config.batch_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    train_filepath = config.train_filepath\n",
    "\n",
    "    # Load TSP data\n",
    "    dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n",
    "    if batches_per_epoch != -1:\n",
    "        batches_per_epoch = min(batches_per_epoch, dataset.max_iter)\n",
    "    else:\n",
    "        batches_per_epoch = dataset.max_iter\n",
    "\n",
    "    # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "    \n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    # running_err_edges = 0.0\n",
    "    # running_err_tour = 0.0\n",
    "    # running_err_tsp = 0.0\n",
    "    running_pred_tour_len = 0.0\n",
    "    running_gt_tour_len = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "\n",
    "    start_epoch = time.time()\n",
    "    for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "        # Generate a batch of TSPs\n",
    "        try:\n",
    "            batch = next(dataset)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)\n",
    "        x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)\n",
    "        x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)\n",
    "        x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "        y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)\n",
    "        y_nodes = Variable(torch.LongTensor(batch.nodes_target).type(dtypeLong), requires_grad=False)\n",
    "        \n",
    "        # Compute class weights (if uncomputed)\n",
    "        if type(edge_cw) != torch.Tensor:\n",
    "            edge_labels = y_edges.cpu().numpy().flatten()\n",
    "            edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        loss = loss.mean()  # Take mean of loss across multiple GPUs\n",
    "        loss = loss / accumulation_steps  # Scale loss by accumulation steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_num+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Compute error metrics and mean tour lengths\n",
    "        # err_edges, err_tour, err_tsp, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "        pred_tour_len = mean_tour_len_edges(x_edges_values, y_preds)\n",
    "        gt_tour_len = np.mean(batch.tour_len)\n",
    "\n",
    "        # Update running data\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size* loss.data.item()* accumulation_steps  # Re-scale loss\n",
    "        # running_err_edges += batch_size* err_edges\n",
    "        # running_err_tour += batch_size* err_tour\n",
    "        # running_err_tsp += batch_size* err_tsp\n",
    "        running_pred_tour_len += batch_size* pred_tour_len\n",
    "        running_gt_tour_len += batch_size* gt_tour_len\n",
    "        running_nb_batch += 1\n",
    "        \n",
    "        # Log intermediate statistics\n",
    "        result = ('loss:{loss:.4f} pred_tour_len:{pred_tour_len:.3f} gt_tour_len:{gt_tour_len:.3f}'.format(\n",
    "            loss=running_loss/running_nb_data,\n",
    "            pred_tour_len=running_pred_tour_len/running_nb_data,\n",
    "            gt_tour_len=running_gt_tour_len/running_nb_data))\n",
    "        master_bar.child.comment = result\n",
    "\n",
    "    # Compute statistics for full epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    err_edges = 0 # running_err_edges/ running_nb_data\n",
    "    err_tour = 0 # running_err_tour/ running_nb_data\n",
    "    err_tsp = 0 # running_err_tsp/ running_nb_data\n",
    "    pred_tour_len = running_pred_tour_len/ running_nb_data\n",
    "    gt_tour_len = running_gt_tour_len/ running_nb_data\n",
    "\n",
    "    return time.time()-start_epoch, loss, err_edges, err_tour, err_tsp, pred_tour_len, gt_tour_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, config, master_bar, mode='test'):\n",
    "    # Set evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Assign parameters\n",
    "    num_nodes = config.num_nodes\n",
    "    num_neighbors = config.num_neighbors\n",
    "    #batch_size = config.batch_size\n",
    "    #batches_per_epoch = config.batches_per_epoch\n",
    "    batch_size =10\n",
    "    batches_per_epoch=1\n",
    "    beam_size = config.beam_size\n",
    "    val_filepath = config.val_filepath\n",
    "    test_filepath = config.test_filepath\n",
    "\n",
    "    # Load TSP data\n",
    "    if mode == 'val':\n",
    "        dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size=batch_size, filepath=val_filepath)\n",
    "    elif mode == 'test':\n",
    "        dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size=batch_size, filepath=test_filepath)\n",
    "    #batches_per_epoch = dataset.max_iter\n",
    "\n",
    "    # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "    \n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    # running_err_edges = 0.0\n",
    "    # running_err_tour = 0.0\n",
    "    # running_err_tsp = 0.0\n",
    "    running_pred_tour_len = 0.0\n",
    "    running_gt_tour_len = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "            # Generate a batch of TSPs\n",
    "            try:\n",
    "                batch = next(dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            # Convert batch to torch Variables\n",
    "            x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)\n",
    "            x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)\n",
    "            x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)\n",
    "            x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "            y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)\n",
    "            y_nodes = Variable(torch.LongTensor(batch.nodes_target).type(dtypeLong), requires_grad=False)\n",
    "            \n",
    "            # Compute class weights (if uncomputed)\n",
    "            if type(edge_cw) != torch.Tensor:\n",
    "                edge_labels = y_edges.cpu().numpy().flatten()\n",
    "                edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "            loss = loss.mean()  # Take mean of loss across multiple GPUs\n",
    "\n",
    "            # Compute error metrics\n",
    "            # err_edges, err_tour, err_tsp, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "\n",
    "            # Get batch beamsearch tour prediction\n",
    "            if mode == 'val': \n",
    "                bs_nodes = beamsearch_tour_nodes(\n",
    "                    y_preds, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
    "            elif mode == 'test': \n",
    "                bs_nodes = beamsearch_tour_nodes_shortest(\n",
    "                    y_preds, x_edges_values, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
    "            \n",
    "            # Compute mean tour length\n",
    "            pred_tour_len = mean_tour_len_nodes(x_edges_values, bs_nodes)\n",
    "            gt_tour_len = np.mean(batch.tour_len)\n",
    "\n",
    "            # Update running data\n",
    "            running_nb_data += batch_size\n",
    "            running_loss += batch_size* loss.data.item()\n",
    "            # running_err_edges += batch_size* err_edges\n",
    "            # running_err_tour += batch_size* err_tour\n",
    "            # running_err_tsp += batch_size* err_tsp\n",
    "            running_pred_tour_len += batch_size* pred_tour_len\n",
    "            running_gt_tour_len += batch_size* gt_tour_len\n",
    "            running_nb_batch += 1\n",
    "\n",
    "            # Log intermediate statistics\n",
    "            result = ('loss:{loss:.4f} pred_tour_len:{pred_tour_len:.3f} gt_tour_len:{gt_tour_len:.3f}'.format(\n",
    "                loss=running_loss/running_nb_data,\n",
    "                pred_tour_len=running_pred_tour_len/running_nb_data,\n",
    "                gt_tour_len=running_gt_tour_len/running_nb_data))\n",
    "            master_bar.child.comment = result\n",
    "\n",
    "    # Compute statistics for full epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    err_edges = 0 # running_err_edges/ running_nb_data\n",
    "    err_tour = 0 # running_err_tour/ running_nb_data\n",
    "    err_tsp = 0 # running_err_tsp/ running_nb_data\n",
    "    pred_tour_len = running_pred_tour_len/ running_nb_data\n",
    "    gt_tour_len = running_gt_tour_len/ running_nb_data\n",
    "\n",
    "    return time.time()-start_test, loss, err_edges, err_tour, err_tsp, pred_tour_len, gt_tour_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):#training & validation\n",
    "    # Instantiate the network\n",
    "    net = nn.DataParallel(ResidualGatedGCNModel(config, dtypeFloat, dtypeLong))\n",
    "    learning_rate = config.learning_rate\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    epoch_bar = master_bar(range(1))\n",
    "    # Training parameters\n",
    "    num_nodes = config.num_nodes\n",
    "    num_neighbors = config.num_neighbors\n",
    "    max_epochs = config.max_epochs\n",
    "    val_every = config.val_every\n",
    "    test_every = config.test_every\n",
    "    batch_size = config.batch_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    learning_rate = config.learning_rate\n",
    "    decay_rate = config.decay_rate\n",
    "    ##train,test,or validation\n",
    "    for epoch in epoch_bar:\n",
    "        train_time, train_loss, train_err_edges, train_err_tour, train_err_tsp, train_pred_tour_len, train_gt_tour_len = train_(net, optimizer, config, epoch_bar)\n",
    "        #test_time, test_loss, test_err_edges, test_err_tour, test_err_tsp, test_pred_tour_len, test_gt_tour_len = test(net, config, epoch_bar, mode='test')\n",
    "        if epoch % val_every == 0 or epoch == max_epochs-1:\n",
    "                # Validate\n",
    "                val_time, val_loss, val_err_edges, val_err_tour, val_err_tsp, val_pred_tour_len, val_gt_tour_len = test(net, config, epoch_bar, mode='val')\n",
    "                # Save checkpoint\n",
    "                if val_pred_tour_len < best_pred_tour_len:\n",
    "                    best_pred_tour_len = val_pred_tour_len  # Update best prediction\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                    }, log_dir+\"best_val_checkpoint.tar\")\n",
    "\n",
    "                # Update learning rate\n",
    "                if val_loss > 0.99 * val_loss_old:\n",
    "                    learning_rate /= decay_rate\n",
    "                    optimizer = update_learning_rate(optimizer, learning_rate)\n",
    "\n",
    "                val_loss_old = val_loss  # Update old validation loss\n",
    "\n",
    "        if epoch % test_every == 0 or epoch == max_epochs-1:\n",
    "            # Test\n",
    "            test_time, test_loss, test_err_edges, test_err_tour, test_err_tsp, test_pred_tour_len, test_gt_tour_len = test(net, config, epoch_bar, mode='test')\n",
    "        if epoch != 0 and (epoch % 250 == 0 or epoch == max_epochs-1):\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                }, log_dir+f\"checkpoint_epoch{epoch}.tar\")\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "log_dir = f\"./logs/{config.expt_name}/\"\n",
    "\n",
    "#checkpoint = torch.load(log_dir+\"tsp20_checkpoint.tar\", map_location='cpu')\n",
    "#checkpoint = torch.load(log_dir+\"tsp50_checkpoint.tar\", map_location='cpu')\n",
    "checkpoint = torch.load(log_dir+\"last_train_checkpoint.tar\", map_location='cpu')\n",
    "#checkpoint = torch.load(log_dir+\"tsp100_checkpoint.tar\", map_location='cpu')\n",
    "# Load network state\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Load optimizer state\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# Load other training parameters\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['train_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "for param_group in optimizer.param_groups:\n",
    "    learning_rate = param_group['lr'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "net.eval()\n",
    "batch_size = 5\n",
    "num_nodes = config.num_nodes\n",
    "num_neighbors = config.num_neighbors\n",
    "#beam_size = config.beam_size\n",
    "beam_size = 100\n",
    "test_filepath = config.test_filepath\n",
    "dataset = iter(GoogleTSPReader(num_nodes, num_neighbors, batch_size, test_filepath))\n",
    "batch = next(dataset)\n",
    "with torch.no_grad():\n",
    "    # Convert batch to torch Variables\n",
    "    x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)\n",
    "    x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)\n",
    "    x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)\n",
    "    x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "    y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)\n",
    "    y_nodes = Variable(torch.LongTensor(batch.nodes_target).type(dtypeLong), requires_grad=False)\n",
    "\n",
    "    # Compute class weights\n",
    "    edge_labels = y_edges.cpu().numpy().flatten()\n",
    "    edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "    # Forward pass\n",
    "    y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Get batch beamsearch tour prediction\n",
    "    config.beam_size =1#then greedy search this is\n",
    "    bs_nodes = beamsearch_tour_nodes_shortest(\n",
    "        y_preds, x_edges_values,  beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
    "    #bs_nodes = beamsearch_tour_nodes(\n",
    "       # y_preds, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
    "   # bs_nodes=np.array(bs_nodes)\n",
    "    #bs_nodes=bs_nodes-1\n",
    "    #bs_nodes=torch.tensor(bs_nodes)\n",
    "    #bs_nodes=bs_nodes.unsqueeze(0)\n",
    "    print(bs_nodes)\n",
    "    # Compute mean tour length\n",
    "    pred_tour_len = mean_tour_len_nodes(x_edges_values, bs_nodes)\n",
    "    gt_tour_len = np.mean(batch.tour_len)\n",
    "    # Plot prediction visualizations\n",
    "    plot_predictions_beamsearch(x_nodes_coord, x_edges, x_edges_values, y_edges, y_preds, bs_nodes, num_plots=batch_size)\n",
    "  \n",
    "     # Get batch iterative algorithm tour prediction\n",
    "    #dm_nodes = dm(\n",
    "        #y_preds, x_edges_values, batch_size, num_nodes)\n",
    "    #print(dm_nodes)\n",
    "    # Compute mean tour length\n",
    "    #pred_tour_len = mean_tour_len_nodes(x_edges_values, bs_nodes)\n",
    "    #gt_tour_len = np.mean(batch.tour_len)\n",
    "    # Plot prediction visualizations\n",
    "    #plot_predictions_beamsearch(x_nodes_coord, x_edges, x_edges_values, y_edges, y_preds, bs_nodes, num_plots=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
